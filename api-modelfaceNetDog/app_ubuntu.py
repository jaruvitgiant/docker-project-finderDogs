import os
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.neighbors import NearestNeighbors
import torch
import numpy as np
import joblib
import shutil
from io import BytesIO
from PIL import Image
from typing import List

from fastapi import FastAPI, Depends, HTTPException, UploadFile, File, APIRouter

# ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Modules ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
# from model.DogImage import Dog, DogImage
from yolo11.yoloCrop import yoloCrop
from resnet.resnet import ResNet, ResNetBackbone, Bottleneck
from model_manager import load_model, ACTIVE_MODEL_NAME ,ACTIVE_MODEL_PATH

import albumentations as A
from albumentations.pytorch import ToTensorV2
from torch.utils.data import Dataset, DataLoader

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
import pickle
import io
from sklearn.decomposition import PCA # ‡πÉ‡∏ä‡πâ‡∏•‡∏î‡∏°‡∏¥‡∏ï‡∏¥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏î‡∏£‡∏π‡∏õ
from pydantic import BaseModel
import base64
from sklearn.manifold import TSNE
import seaborn as sns
from pathlib import Path

app = FastAPI()
router = APIRouter()
device = "cuda" if torch.cuda.is_available() else "cpu"
# --- Global Variables ---
# model152 = None 
model152 = ResNetBackbone(Bottleneck, [3, 8, 36, 3], embedding_size=512)
ACTIVE_MODEL_NAME = "None"
DEFAULT_MODEL_PATH = "/app/resnet/model/resne152-V01_60.pt"

# ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á Check ‡∏Å‡πà‡∏≠‡∏ô‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏£‡∏¥‡∏á (‡∏ñ‡πâ‡∏≤‡∏û‡∏±‡∏á‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô Log ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô)
if os.path.exists(DEFAULT_MODEL_PATH):
    print(f"‚úÖ Found model at: {DEFAULT_MODEL_PATH}")
else:
    print(f"‚ùå CANNOT FIND model at: {DEFAULT_MODEL_PATH}")
    # ‡∏•‡∏≠‡∏á List ‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏≠‡∏ö‡πÜ ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏î‡∏π‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠
    print(f"Files in /app/resnet/model: {os.listdir('/app/resnet/model')}")

def load_model_engine(path: str):
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Å‡∏•‡∏≤‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏´‡∏•‡∏î Weight ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ Global Model"""
    global model152, ACTIVE_MODEL_NAME
    try:
        if not os.path.exists(path):
            print(f" Error: Model path {path} not found.")
            return False
        
        # ‡πÇ‡∏´‡∏•‡∏î state_dict
        state_dict = torch.load(path, map_location=device)
        
        # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á model152 ‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß
        # model152.load_state_dict(state_dict) 
        
        # ‡∏´‡∏≤‡∏Å num_classes ‡πÉ‡∏ô‡∏£‡∏∏‡πà‡∏ô‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏°‡πà‡πÄ‡∏ó‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏° ‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏≤‡∏à‡∏ï‡πâ‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î meta.json ‡∏°‡∏≤‡πÄ‡∏ä‡πá‡∏Ñ‡∏Å‡πà‡∏≠‡∏ô
        # ‡πÅ‡∏ï‡πà‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡∏ñ‡πâ‡∏≤‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏° ‡πÉ‡∏ä‡πâ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ:
        model152.load_state_dict(state_dict)
        model152.to(device)
        model152.eval() # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å: ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏õ‡πá‡∏ô eval mode ‡πÄ‡∏™‡∏°‡∏≠

        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ä‡∏∑‡πà‡∏≠‡∏£‡∏∏‡πà‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà (‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠ Folder ‡∏°‡∏≤‡∏à‡∏≤‡∏Å Path)
        ACTIVE_MODEL_NAME = os.path.basename(os.path.dirname(path))
        print(f" Model updated to: {path}")
        return True
    except Exception as e:
        print(f"Failed to load model: {e}")
        return False
    
@router.post("/select-model")
def select_model(version: str):
    # 1. ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏ñ‡πâ‡∏≤‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "default" ‡∏°‡∏≤ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ Path ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏ß‡πâ
    if version == "default":
        model_path = DEFAULT_MODEL_PATH
    else:
        # 2. ‡∏ñ‡πâ‡∏≤‡∏™‡πà‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏°‡∏≤ (‡πÄ‡∏ä‡πà‡∏ô V01, V02) ‡πÉ‡∏´‡πâ‡πÑ‡∏õ‡∏´‡∏≤‡πÉ‡∏ô checkpoints
        model_path = os.path.join(BASE_CHECKPOINT_DIR, version, "model.pth")

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡πÑ‡∏´‡∏°
    if not os.path.exists(model_path):
        raise HTTPException(
            status_code=404, 
            detail=f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏•: {version} (‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ó‡∏µ‡πà: {model_path})"
        )

    # ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏Ç‡πâ‡∏≤‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏¢‡∏ô‡∏ï‡πå (Global model152)
    success = load_model_engine(model_path)
    
    if not success:
        raise HTTPException(status_code=500, detail="‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î Weights")

    return {
        "status": "success",
        "active_model": version,
        "path": model_path
    }

@router.get("/current-model")
def current_model():
    return {
        "active_model": ACTIVE_MODEL_NAME,
        "device": str(device)
    }


@router.get("/models")
def list_models():
    models = []

    # üîπ default model
    models.append({
        "id": "default",
        "type": "legacy",
        "path": DEFAULT_MODEL_PATH,
        "active": ACTIVE_MODEL_PATH == DEFAULT_MODEL_PATH
    })

    # üîπ versioned models
    if os.path.exists(BASE_CHECKPOINT_DIR):
        for version_dir in sorted(os.listdir(BASE_CHECKPOINT_DIR)):
            version_path = os.path.join(BASE_CHECKPOINT_DIR, version_dir)
            model_path = os.path.join(version_path, "model.pth")
            meta_path = os.path.join(version_path, "meta.json")

            if not os.path.exists(model_path):
                continue

            info = {
                "id": version_dir,
                "type": "versioned",
                "path": model_path,
                "active": ACTIVE_MODEL_PATH == model_path
            }

            if os.path.exists(meta_path):
                try:
                    with open(meta_path, "r", encoding="utf-8") as f:
                        info["details"] = json.load(f)
                except json.JSONDecodeError:
                    info["details"] = {"error": "invalid meta.json"}

            models.append(info)

    return {"models": models}

@app.on_event("startup")
def startup_event():
    global ACTIVE_MODEL_NAME
    print(f"üöÄ System starting up... Loading default model from {DEFAULT_MODEL_PATH}")
    
    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô engine ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏ß‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÇ‡∏´‡∏•‡∏î Weights ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
    success = load_model_engine(DEFAULT_MODEL_PATH)
    
    if success:
        # ‡∏ñ‡πâ‡∏≤‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡πÉ‡∏´‡πâ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
        ACTIVE_MODEL_NAME = os.path.basename(DEFAULT_MODEL_PATH)
        print(f"‚úÖ Default model loaded successfully: {ACTIVE_MODEL_NAME}")
    else:
        print(f"‚ùå Failed to load default model. Please check PATH1.")

## --- Dataset ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏π‡∏õ‡∏à‡∏≤‡∏Å path list ---------------------------------------------

transform = A.Compose([
    A.Resize(224, 224),
    A.Normalize(
        mean=[0.485, 0.456, 0.406], 
        std=[0.229, 0.224, 0.225]
    ),
    ToTensorV2(),
])
class AutoDogPipelineDatasetFromList(Dataset):
    def __init__(self, image_paths, transform=None):
        self.image_paths = image_paths
        self.transform = transform

        if len(self.image_paths) == 0:
            print("Warning: No images provided to dataset")

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        full_path = self.image_paths[idx]

        # --- YOLO Crop ---
        cropped_img = yoloCrop(full_path)

        if cropped_img is None:
            cropped_img = Image.open(full_path).convert("RGB")

        # --- Transform ---
        if self.transform:
            if isinstance(cropped_img, Image.Image):
                image_np = np.array(cropped_img)
            else:
                image_np = cropped_img

            augmented = self.transform(image=image_np)
            image_tensor = augmented["image"]
        else:
            image_tensor = torch.from_numpy(np.array(cropped_img)).permute(2, 0, 1).float()

        return image_tensor

def get_embedding(img_pil, model, transform, device):
    """‡πÅ‡∏õ‡∏•‡∏á PIL Image ‡πÄ‡∏õ‡πá‡∏ô Embedding Vector"""
    model.eval()
    image_np = np.array(img_pil)
    augmented = transform(image=image_np)
    image_tensor = augmented['image'].unsqueeze(0).to(device)
    
    with torch.no_grad():
        embedding = model(image_tensor).cpu().numpy().flatten()
    return embedding

from fastapi import FastAPI, UploadFile, File, Form, HTTPException
@app.post("/embedding-image/")
async def embedding_image(
    dog_id: int = Form(...),
    files: List[UploadFile] = File(...)
):
    try:
        results = []
        processed = 0

        for file in files:
            contents = await file.read()
            img_pil = Image.open(io.BytesIO(contents)).convert("RGB")

            # YOLO crop & Embedding logic
            cropped_img = yoloCrop(img_pil) or img_pil
            emb = get_embedding(cropped_img, model152, transform, device) # ‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô numpy array

            # ‡πÅ‡∏õ‡∏•‡∏á embedding ‡πÄ‡∏õ‡πá‡∏ô base64 string
            # ‡πÉ‡∏ä‡πâ .astype(np.float32) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏∏‡∏°‡∏Ç‡∏ô‡∏≤‡∏î data type ‡πÉ‡∏´‡πâ‡∏Ñ‡∏á‡∏ó‡∏µ‡πà
            emb_bytes = emb.astype(np.float32).tobytes()
            emb_base64 = base64.b64encode(emb_bytes).decode('utf-8')

            # (Optional) ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï DB ‡∏ù‡∏±‡πà‡∏á API ‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
            # DogImage.objects.filter(...)

            processed += 1
            results.append({
                "filename": file.filename,
                "embedding_dim": len(emb),
                "embedding_base64": emb_base64  # ‡∏™‡πà‡∏á‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ
            })

        return {
            "dog_id": dog_id,
            "processed": processed,
            "results": results
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
 
import uuid
from fastapi import HTTPException

@app.post("/SEARCH-DOG02/")
async def search_dog02(file: UploadFile = File(...)):
    """‡∏£‡∏±‡∏ö‡∏£‡∏π‡∏õ‡πÉ‡∏´‡∏°‡πà ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Top-3 ‡∏à‡∏≤‡∏Å KNN ‡∏ó‡∏µ‡πà‡πÄ‡∏ã‡∏ü‡πÑ‡∏ß‡πâ"""

    if not os.path.exists('models/knn_latest02.joblib'):
        raise HTTPException(
            status_code=400,
            detail="KNN model not trained yet. Please run /TRAIN-KNN/ first."
        )

    # ‡∏≠‡πà‡∏≤‡∏ô‡∏£‡∏π‡∏õ
    contents = await file.read()
    img_pil = Image.open(BytesIO(contents)).convert("RGB")

    # YOLO Crop
    cropped_test_img = yoloCrop(img_pil)

    # ‡∏ñ‡πâ‡∏≤ YOLO ‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‚Üí return ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
    if cropped_test_img is None:
        
        return {
            "status": "not_found",
            "message": "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏°‡∏≤‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö (YOLO ‡∏ï‡∏£‡∏ß‡∏à‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏™‡∏∏‡∏ô‡∏±‡∏Ç‡πÉ‡∏ô‡∏†‡∏≤‡∏û)",
            "results": []
        }

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà crop ‡πÑ‡∏î‡πâ
    save_dir = "search_history"
    os.makedirs(save_dir, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"crop_{timestamp}_{uuid.uuid4().hex[:6]}.jpg"
    save_path = os.path.join(save_dir, filename)
    cropped_test_img.save(save_path)

    print(f"Saved cropped image to: {save_path}")

    # Transform ‚Üí Tensor
    test_tensor = transform(image=np.array(cropped_test_img))['image']
    test_tensor = test_tensor.unsqueeze(0).to(device)

    # Extract embedding
    model152.eval()
    with torch.no_grad():
        test_embedding = model152(test_tensor).cpu().numpy()

    #  Load KNN
    knn = joblib.load("models/knn_latest02.joblib")
    filenames = joblib.load("models/labels_latest02.joblib")

    #  Search
    distances, indices = knn.kneighbors(test_embedding)
    #print("DEBUG distances:", distances)
    #print("DEBUG indices:", indices)
    
    #  Result
    unique_results = {}

    for i, idx in enumerate(indices[0]):
        dog_id = filenames[idx]
        current_distance = float(distances[0][i])
        
        # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ id ‡∏ô‡∏µ‡πâ‡πÉ‡∏ô dict ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏à‡∏≠‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á (distance) ‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°
        if dog_id not in unique_results or current_distance < unique_results[dog_id]:
            unique_results[dog_id] = current_distance

    # ‡∏ô‡∏≥‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å dict ‡∏°‡∏≤‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô list ‡∏Ç‡∏≠‡∏á objects
    sorted_results = [
        {"dog_id": dog_id, "distance": dist} 
        for dog_id, dist in unique_results.items()
    ]

    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏° distance ‡∏à‡∏≤‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÑ‡∏õ‡∏°‡∏≤‡∏Å ‡πÅ‡∏•‡∏∞‡πÄ‡∏≠‡∏≤‡πÅ‡∏Ñ‡πà 5 ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å
    sorted_results = sorted(sorted_results, key=lambda x: x["distance"])[:5]

    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö rank ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß
    final_results = []
    for i, res in enumerate(sorted_results):
        final_results.append({
            "rank": i + 1,
            "dog_id": res["dog_id"],
            "distance": res["distance"]
        })

    return {"results": final_results}

import asyncio
import json
import threading
import os
from datetime import datetime
from fastapi import FastAPI, BackgroundTasks, Depends
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from sklearn.preprocessing import LabelEncoder
from middleware.auth import verify_token
from resnet.train import FaceModelTrainer
from resnet.DataLoader import get_dataloaders

# --- 1. ‡πÄ‡∏û‡∏¥‡πà‡∏° CORS Middleware (‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ) ---
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://127.0.0.1:8000", "http://localhost:8000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Queue ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö SSE Clients
train_status_queues = set()

async def broadcast_status(data):
    """‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Ç‡πâ‡∏≤ Queue ‡∏Ç‡∏≠‡∏á SSE ‡∏ó‡∏∏‡∏Å‡∏≠‡∏±‡∏ô"""
    if train_status_queues:
        if isinstance(data, str):
            data = {"status": data}
        formatted_msg = f"data: {json.dumps(data)}\n\n"
        await asyncio.gather(*[q.put(formatted_msg) for q in train_status_queues])

# ---   ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏ó‡∏£‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÉ‡∏ô Thread ‡πÅ‡∏¢‡∏Å ---

def get_next_version(base_dir: str):
    os.makedirs(base_dir, exist_ok=True)

    versions = [
        d for d in os.listdir(base_dir)
        if d.startswith("v") and d[1:].isdigit()
    ]

    if not versions:
        return "v001"

    latest = max(int(v[1:]) for v in versions)
    return f"v{latest + 1:03d}"
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• ResNet152 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏™‡∏≠‡∏ô
train_model = ResNetBackbone(Bottleneck, [3, 8, 36, 3],embedding_size=512).to(device)

class TrainingItem(BaseModel):
    image_path: str
    label: int

class RetrainRequest(BaseModel):
    data: List[TrainingItem]
    model_type: str
def background_train_task(loop, training_data: List[TrainingItem]):
    
    def sse_callback(progress_data):
        # ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏´‡∏≤ Event Loop ‡∏Ç‡∏≠‡∏á FastAPI
        asyncio.run_coroutine_threadsafe(broadcast_status(progress_data), loop)

    try:
        if not training_data:
            sse_callback("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å Django")
            return

        sse_callback(f"üì¶ ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {len(training_data)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Å‡∏≤‡∏£ Crop...")

        cropped_img_list = []
        labels = [] 
        
        # 1. ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å Payload ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö
        for item in training_data:
            full_path = item.image_path # ‡πÉ‡∏ä‡πâ path ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å Django
            
            if not os.path.exists(full_path):
                print(f"File not found: {full_path}")
                continue

            # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û (YOLO Crop)
            # ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: result ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡πá‡∏ô Image object ‡∏´‡∏£‡∏∑‡∏≠ Path ‡∏ó‡∏µ‡πà crop ‡πÅ‡∏•‡πâ‡∏ß‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà get_dataloaders ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            result = yoloCrop(full_path) 
            
            if result is None:
                continue

            cropped_img_list.append(result)
            labels.append(item.label)

        if not cropped_img_list:
            sse_callback("‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏£ Crop (YOLO ‡∏´‡∏≤‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠)")
            return

        # 2. Label Encoding
        sse_callback("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏à‡∏±‡∏î‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Labels...")
        le = LabelEncoder()
        encoded_labels = le.fit_transform(labels) 
        actual_num_classes = len(le.classes_) 

        # 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á DataLoader (‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà Crop ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ)
        train_loader, _ = get_dataloaders(
            train_path=cropped_img_list,
            image_ids=encoded_labels, 
            batch_size=min(32, len(cropped_img_list)) # ‡∏õ‡∏£‡∏±‡∏ö batch_size ‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡∏¥‡∏ô‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏π‡∏õ
        )

        # 4. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Path ‡πÅ‡∏•‡∏∞ Version
        BASE_CHECKPOINT_DIR = "checkpoints/resnet152"
        version = get_next_version(BASE_CHECKPOINT_DIR)
        save_path = os.path.join(BASE_CHECKPOINT_DIR, version)
        os.makedirs(save_path, exist_ok=True)

        # 5. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏™‡∏≠‡∏ô
        sse_callback(f"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏ó‡∏£‡∏ô Model {version} ({actual_num_classes} ‡∏Ñ‡∏•‡∏≤‡∏™)...")
        
        # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ train_model ‡∏ñ‡∏π‡∏Å‡∏ô‡∏¥‡∏¢‡∏≤‡∏°‡πÑ‡∏ß‡πâ‡∏Ç‡πâ‡∏≤‡∏á‡∏ô‡∏≠‡∏Å ‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö output layer ‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô class ‡∏à‡∏£‡∏¥‡∏á
        # model = build_model(num_classes=actual_num_classes) 
        
        trainer = FaceModelTrainer(
            model=train_model, 
            train_loader=train_loader,
            device=device,
            num_classes=actual_num_classes,
            embedding_size=512
        )
        
        trainer.train(epochs=3, save_path=save_path, progress_callback=sse_callback)

        # 6. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Model ‡πÅ‡∏•‡∏∞ Metadata
        torch.save(train_model.state_dict(), os.path.join(save_path, "model.pth"))

        meta = {
            "model": "resnet152",
            "version": version,
            "trained_at": datetime.now().isoformat(),
            "num_classes": int(actual_num_classes),
            "classes": [int(c) for c in le.classes_], 
            "num_images": len(cropped_img_list),
            "epochs": 3,
            "device": str(device)
        }

        with open(os.path.join(save_path, "meta.json"), "w", encoding="utf-8") as f:
            json.dump(meta, f, indent=2, ensure_ascii=False)

        sse_callback(f"‚úÖ ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏™‡∏≠‡∏ô {version} ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!")

    except Exception as e:
        sse_callback(f"‚ùå Error: {str(e)}")
        print(f"Error during training: {e}")

# --- 3. API Endpoints ---

@app.get("/train-progress")
async def stream_training_progress():
    """Endpoint ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Log (SSE)"""
    queue = asyncio.Queue()
    train_status_queues.add(queue)
    
    async def event_generator():
        try:
            yield f"data: {json.dumps({'status': 'üì° ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Server ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à'})}\n\n"
            while True:
                data = await queue.get()
                yield data
        except asyncio.CancelledError:
            train_status_queues.remove(queue)

    return StreamingResponse(event_generator(), media_type="text/event-stream")
from pydantic import BaseModel
from typing import List


@app.post("/retrain-model-face")
async def retrain(request: RetrainRequest, payload=Depends(verify_token)):
    """API ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏ó‡∏£‡∏ô ‡πÇ‡∏î‡∏¢‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• List ‡∏Ç‡∏≠‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞ Label"""
    
    loop = asyncio.get_running_loop()
    
    # ‡∏™‡πà‡∏á request.data (List ‡∏Ç‡∏≠‡∏á TrainingItem) ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô background task
    thread = threading.Thread(
        target=background_train_task, 
        args=(loop, request.data)
    )
    thread.start()
    
    return {
        "message": "Training started in background",
        "received_items": len(request.data)
    }

## -----------------------test model knn classification ---------------------------


class EmbeddingItem(BaseModel):
    dog_id: int
    embedding_b64: str

class TrainRequest(BaseModel):
    data: List[EmbeddingItem]

@app.post("/tiger_knnTrain/")
async def train_knn02(request: TrainRequest):
    X = []
    y = []

    # 2. ‡πÅ‡∏Å‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Request Body
    for item in request.data:
        try:
            # ‡πÅ‡∏õ‡∏•‡∏á Base64 ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô bytes -> ‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô numpy array
            binary_data = base64.b64decode(item.embedding_b64)
            emb = np.frombuffer(binary_data, dtype=np.float32)

            # Sanity check
            if emb.ndim != 1 or emb.shape[0] == 0:
                continue

            X.append(emb)
            y.append(item.dog_id)
        except Exception as e:
            print(f"Error processing dog_id {item.dog_id}: {e}")
            continue

    if len(X) == 0:
        raise HTTPException(status_code=400, detail="No valid embeddings provided")

    # 3. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞ Train
    X_array = np.vstack(X)

    # ‡πÉ‡∏ä‡πâ Cosine Metric ‡∏ï‡∏≤‡∏°‡πÄ‡∏î‡∏¥‡∏°
    knn = NearestNeighbors(
        n_neighbors=len(X_array),
        # n_neighbors=min(3, len(X_array)),
        metric="cosine"
    )
    knn.fit(X_array)

    # 4. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Model
    os.makedirs("models", exist_ok=True)
    joblib.dump(knn, "models/knn_latest02.joblib")
    joblib.dump(y, "models/labels_latest02.joblib")

    return {
        "status": "success",
        "total_embeddings_trained": len(X_array)
    }

def create_plot_64(X_transformed, y, title, xlabel, ylabel):
    # ‡πÉ‡∏ä‡πâ‡∏´‡∏°‡∏ß‡∏î Agg ‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡∏•‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á Thread ‡∏Ç‡∏≠‡∏á Tkinter
    plt.figure(figsize=(10, 7))
    try:
        scatter = plt.scatter(X_transformed[:, 0], X_transformed[:, 1], c=y, cmap='viridis', edgecolors='k', alpha=0.7)
        plt.colorbar(scatter, label='Dog ID')
        plt.title(title)
        plt.xlabel(xlabel)
        plt.ylabel(ylabel)
        
        buf = io.BytesIO()
        plt.savefig(buf, format='png', bbox_inches='tight')
        buf.seek(0)
        img_b64 = base64.b64encode(buf.read()).decode('utf-8')
        return img_b64
    finally:
        # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å: ‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏¥‡∏î plot ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏∑‡∏ô Memory ‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Thread
        plt.clf()
        plt.close('all')

@app.post("/test-knn/")
async def test_knn(request: TrainRequest):
    try:
        embeddings = []
        labels = []

        #  Decode ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        for item in request.data:
            binary_data = base64.b64decode(item.embedding_b64)
            vector = np.frombuffer(binary_data, dtype=np.float32) 
            embeddings.append(vector)
            labels.append(item.dog_id)

        X = np.array(embeddings)
        y = np.array(labels)

        if len(X) < 2:
            raise ValueError("Data points must be at least 2 for visualization.")

        # t-SNE ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏î‡∏π‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Å‡∏•‡∏∏‡πà‡∏° (Cluster) ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏¥‡∏ï‡∏¥‡∏™‡∏π‡∏á
        perplexity = min(20, len(X) - 1)
        tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)
        X_tsne = tsne.fit_transform(X)
        tsne_image = create_plot_64(X_tsne, y, "t-SNE Visualization of Dog Embeddings", "t-SNE dimension 1", "t-SNE dimension 2")

        # ---  KNN Confusion Matrix ---
        knn = KNeighborsClassifier(n_neighbors=2)
        knn.fit(X, y)
        y_pred = knn.predict(X)

        #  ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤ Accuracy
        acc = accuracy_score(y, y_pred)

        #  ‡∏™‡∏£‡πâ‡∏≤‡∏á Confusion Matrix (‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏î‡∏¥‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì)
        cm = confusion_matrix(y, y_pred)
        unique_labels = np.unique(y)

        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                     xticklabels=unique_labels, 
                    yticklabels=unique_labels)
        plt.title(f"KNN Confusion Matrix (Accuracy: {acc:.2f})") # ‡πÅ‡∏™‡∏î‡∏á Accuracy ‡πÉ‡∏ô Title ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡πá‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏±‡∏ö
        plt.ylabel('Actual Dog ID')
        plt.xlabel('Predicted Dog ID')

        buf = io.BytesIO()
        plt.savefig(buf, format='png', bbox_inches='tight')
        buf.seek(0)
        knn_matrix_image = base64.b64encode(buf.read()).decode('utf-8')
        plt.clf()
        plt.close('all')

        return {
            "status": "success",
            "accuracy": float(acc), # ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤ accuracy ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ (‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô float ‡∏õ‡∏Å‡∏ï‡∏¥)
            "tsne_plot": tsne_image,
            "knn_matrix": knn_matrix_image,
            "model_name": ACTIVE_MODEL_NAME
        }

    except Exception as e:
        plt.close('all')
        raise HTTPException(status_code=500, detail=str(e))
    
app.include_router(router)
